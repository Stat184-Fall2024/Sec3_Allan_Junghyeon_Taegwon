---
title: "[Stat 184] Final Project"
subtitle: Netflix Content Analyis 
author:
  - name: Allan Samoilovich
    corresponding: true
  - name: Junghyeon Sung
    corresponding: true
  - name: Taegwon Lee
    corresponding: true
date: 12/18/2024
format: pdf
echo: false
warning: false
---

# Introdcution

"Netflix" is the most popular media platform these days. It would not be an exaggeration to call it a pioneer of trends. Netflix has revolutionized the way people consume entertainment by introducing binge-watching, personalized recommendations, and producing high-quality original content. Its innovative approach to streaming has set a new standard for the industry, influencing how competitors deliver and create content. As media consumption continues to rise, we chose a netflix dataset to explore Netflix's trends.

## Dataset

We found our dataset from Kaggle and the dataset contains metadata about Netflix's movie and TV shows. It provides a comprehensive view of the Netflix's catalog and is widely used for research and analysis in media trends.

<Should be included>

1.  Describe the provenance of the data \* Where did you get the data \* Who collected the data \* For what purpose \* who/what make up the cases

2.  Explain how data meet the FAIR and/or CARE Principles

-   FAIR Principles:
    -   Findability: Other researchers should be able to find the data - i.e. name files descriptively based on the project. EX 'netflix_titles.csv'
    -   Accessibility: Other researchers should be able to access the data, within reason
    -   Interpretability: Data should use standardized formats/terminologies
    -   Reusability: Data should include detailed metadata to allow reuse

3.  Describe what attributes you'll focus your analysis on

-   Figures and Tables should have appropriate captions and appropriately cross-referenced in the body of your report.

## Data Wrangling

```{r}
# Load dataset
file_path <- "netflix_titles.csv"
netflix_data <- read.csv(file_path)
View(netflix_data)
```

```{r}
summary(netflix_data)
```

```{r}
# Necessary libraries for the entire code
library(dplyr)
library(tidyr)
library(ggplot2)
library(tm)
library(wordcloud)
```

### Handling Missing Values

```{r}
# Find Missing Data
netflix_data[netflix_data == ""] <- NA
netflix_data[netflix_data == " "] <- NA

#colSums(is.na(netflix_data))

# Calculate the number of missing values in each column
missing_data <- colSums(is.na(netflix_data))
print(missing_data)
missing_data_df <- data.frame(Column = names(missing_data), MissingCount = missing_data)

ggplot(missing_data_df, aes(x = reorder(Column, -MissingCount), y = MissingCount, fill = MissingCount)) +
  geom_bar(stat = "identity") +  
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Missing Data Count by Column",
    x = "Columns",
    y = "Count of Missing Values"
  ) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme(legend.position = "none")
```

-   This graph shows that there are a lot of missing values in 'director' column.

```{r}
# Cleaning Missing Data
  # Replace missing values with "Unknown"
netflix_data$director[is.na(netflix_data$director)] <- "Unknown"
netflix_data$cast[is.na(netflix_data$cast)] <- "Unknown"
netflix_data$country[is.na(netflix_data$country)] <- "Unknown"

colSums(is.na(netflix_data))
```

```{r}
# Save the cleaned dataset
write.csv(netflix_data, "cleaned_netflix_data.csv", row.names = FALSE)
cleaned_data <- read.csv("cleaned_netflix_data.csv")
```

## Exploratory Data Analysis

### 1. Movie vs. TV show

```{r}
#| fig-cap: "Content Trends by Bar graph"
# 1. Bar graph
ggplot(cleaned_data, aes(x = type, fill = type)) +
  geom_bar() +
  scale_fill_manual(values = c("#e50914", "#b20710")) +
  labs(title = "Distribution of Content Types", x = "Type", y = "Count") +
  theme_minimal()
```

```{r}
#| fig-cap: "Content Trends by Pie Chart"
# Calculate percentages
content_type_dist <- cleaned_data %>%
  count(type) %>%
  mutate(percentage = n / sum(n) * 100)
# Visualization
ggplot(content_type_dist, aes(x = "", y = n, fill = type)) +
  geom_bar(width = 1, stat = "identity") +  # Bar chart as base
  coord_polar(theta = "y") +                # Convert to pie chart
  scale_fill_manual(values = c("#e50914", "#b20710")) +
  geom_text(aes(label = paste0(round(percentage, 1), "%")),  # Add percentage labels
            position = position_stack(vjust = 0.5)) +     # Center labels
  labs(title = "Distribution of Content Types", x = NULL, y = NULL) +
  theme_minimal() +
  theme(axis.text.x = element_blank(),          # Remove x-axis labels
        axis.ticks = element_blank(),
        panel.grid = element_blank())           # Remove grid lines
```

### 2. Duration

```{r}
#| fig-cap: "Distribution of movie durations in Netflix's catalog."
# Extract numeric values from duration
cleaned_data$duration_numeric <- as.numeric(gsub("[^0-9]", "", cleaned_data$duration))

# Distribution of duration for movies
movie_duration <- cleaned_data %>%
  filter(type == "Movie")

ggplot(movie_duration, aes(x = duration_numeric)) +
  geom_histogram(binwidth = 10, fill = "#e50914", color = "white") +
  labs(title = "Distribution of Movie Durations", x = "Duration (minutes)", y = "Count") +
  theme_minimal()

```

-   The histogram highlights the most common movie lengths, with the majority clustering around 100 minutes.

### 3. Yearly Content

```{r}
#| fig-cap: "Yearly content addition on Netflix over time."
# Yearly trend
cleaned_data$year_added <- as.numeric(format(as.Date(cleaned_data$date_added, "%B %d, %Y"), "%Y"))

# Year-wise content count
yearly_content <- cleaned_data %>%
  filter(!is.na(year_added)) %>%
  count(year_added)

# Line plot
ggplot(yearly_content, aes(x = year_added, y = n)) +
  geom_line(color = "#e50914", size = 1.2) +
  geom_point(color = "#b20710", size = 3) +
  labs(title = "Yearly Content Addition", x = "Year", y = "Content Count") +
  theme_minimal()

```

### 4. Top 10 Genre

```{r}
#| fig-cap: "Genre Trends"

genre_data <- cleaned_data %>%
  separate_rows(listed_in, sep = ", ") %>%
  count(listed_in, sort = TRUE) %>%
  slice_max(order_by = n, n = 10)

# Plot top genres
ggplot(genre_data, aes(x = reorder(listed_in, n), y = n, fill = listed_in)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("#221f1f", "#b20710", "#e50914", "#f5f5f1",
                               "#8c8c8c", "#d4d4d4", "#660000", "#cc0000",
                               "#990033", "#330000")) +
  coord_flip() +
  labs(title = "Top 10 Genres on Netflix", x = "Genre", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

### 5. Top 10 Country

```{r}
#| fig-cap: "Top 10 Countries by Content Count"

# Find the top 15 countries 
top_countries <- cleaned_data %>%
  count(country, sort = TRUE) %>%
  top_n(10, n)
  # Visualization
custom_colors <- c("#e50914", "#221f1f", "#f5f5f1", "#8c8c8c",
                   "#660000", "#990033", "#330000",
                   "#ff5733", "#c70039", "#581845")

ggplot(top_countries, aes(x = reorder(country, n), y = n, fill = country)) + 
  geom_bar(stat = "identity") +
  scale_fill_manual(values = custom_colors) +  # Apply custom colors
  coord_flip() +
  labs(title = "Top 10 Countries by Content Count", x = "Country", y = "Count") +
  theme_minimal()+
  theme(legend.position = "none")
```

# Research Questions

## RQ1. What are the most frequently occurring keywords in the 'description' text of Netflix content?

```{r}
#| fig-cap: "Most frequent words in description"
# Extract the 'description' column and remove missing values
descriptions <- cleaned_data$description
descriptions <- na.omit(descriptions)

# Combine all descriptions into a single text
text <- paste(descriptions, collapse = " ")

# Create a corpus
corpus <- Corpus(VectorSource(text))

# Preprocess the text: remove punctuation, numbers, stopwords, and convert to lowercase
corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english"))

# Create a term-document matrix
tdm <- TermDocumentMatrix(corpus)
tdm_matrix <- as.matrix(tdm)

# Sum the frequency of words
word_freq <- sort(rowSums(tdm_matrix), decreasing = TRUE)

# Create a data frame of words and their frequencies
word_data <- data.frame(word = names(word_freq), freq = word_freq)

# Generate the word cloud
set.seed(123) # For consistent result
wordcloud(
  words = word_data$word,
  freq = word_data$freq,
  min.freq = 3, # Set minimum frequency for words to appear
  max.words = 150, # Set the maximum number of words to display
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)

# Preprocessing to find correlation between directors and keywords in descriptions

# Step 1: Extract and preprocess data
cleaned_data$description <- tolower(cleaned_data$description) 
cleaned_data$description <- gsub("[[:punct:]]", "", cleaned_data$description) 
cleaned_data$description <- gsub("[[:digit:]]", "", cleaned_data$description) 
cleaned_data$description <- gsub("\\s+", " ", cleaned_data$description) 
cleaned_data$description <- trimws(cleaned_data$description) 

# Remove stopwords from descriptions
stop_words <- stopwords("en") 
cleaned_data$processed_description <- sapply(cleaned_data$description, function(x) {
  paste(setdiff(unlist(strsplit(x, " ")), stop_words), collapse = " ")
})

# Step 2: Create a mapping of directors to their descriptions
director_keywords <- cleaned_data %>%
  filter(director != "Unknown") %>%
  group_by(director) %>%
  summarise(all_descriptions = paste(processed_description, collapse = " ")) %>%
  ungroup()

# Step 3: Tokenize and extract keywords for each director
corpus <- Corpus(VectorSource(director_keywords$all_descriptions))

# Further preprocessing
corpus <- corpus %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english"))

# Create a Term-Document Matrix
tdm <- TermDocumentMatrix(corpus)
tdm_matrix <- as.matrix(tdm)

# Find the most frequent terms for each director
director_keywords$top_keywords <- apply(tdm_matrix, 2, function(x) {
  top_terms <- names(sort(x, decreasing = TRUE)[1:10]) # Top 10 terms
  paste(top_terms, collapse = ", ")
})

# Step 4: Visualize the correlations
selected_director <- "Quentin Tarantino" # Replace with any director of interest

# Find the corresponding keywords
if (selected_director %in% director_keywords$director) {
  selected_keywords <- tdm_matrix[, which(director_keywords$director == selected_director)]
  selected_keywords <- selected_keywords[selected_keywords > 0]
  
  # Create a word cloud
  wordcloud(
    words = names(selected_keywords),
    freq = selected_keywords,
    min.freq = 1,
    max.words = 100,
    random.order = FALSE,
    colors = brewer.pal(8, "Dark2")
  )
} else {
  cat("Director not found in the dataset.")
}

# Step 5: Save results
write.csv(director_keywords, "director_keywords.csv", row.names = FALSE)

## RQ2. Which genres are the most frequently produced on Netflix?

```{r}

#| fig-cap: "Top 10 most frequently produced genres on Netflix"

# Step 1: Extract and preprocess data
# Load the 'listed_in' column, which contains genre information, and remove missing values
genres <- cleaned_data$listed_in
genres <- na.omit(genres)

# Step 2: Split multiple genres in a single entry into separate rows
# Split genres by comma and trim whitespace
library(tidyr)
library(dplyr)
genres_data <- data.frame(genres = unlist(strsplit(as.character(genres), ",\\s*")))

# Step 3: Count the frequency of each genre
genre_frequency <- genres_data %>%
  group_by(genres) %>%
  summarise(Frequency = n()) %>%
  arrange(desc(Frequency))

# Display the top 10 genres
print(head(genre_frequency, 10))

# Step 4: Visualize the top genres
library(ggplot2)
ggplot(data = head(genre_frequency, 10), aes(x = reorder(genres, Frequency), y = Frequency)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Top 10 Most Frequently Produced Genres on Netflix",
       x = "Genre",
       y = "Frequency") +
  theme_minimal()

# Step 5: Save the result for further use
write.csv(genre_frequency, "netflix_genre_frequency.csv", row.names = FALSE)

# Additional Exploration: Genres across regions
# If there are columns related to country or region, we can analyze genre distributions by region
if ("country" %in% colnames(cleaned_data)) {
  genre_region <- cleaned_data %>%
    select(listed_in, country) %>%
    separate_rows(listed_in, sep = ",\\s*") %>%
    group_by(country, listed_in) %>%
    summarise(Frequency = n()) %>%
    arrange(country, desc(Frequency))

  # Save region-specific genre data
  write.csv(genre_region, "netflix_genre_by_region.csv", row.names = FALSE)
}

```

### RQ1 Findings

By analyzing frequency of text data efficiently, we used 'word cloud' graph. From research question 1, we found that the word "life" appears prominently, suggesting that many descriptions revolve around stories about human experiences. Similarly, the frequent occurrence of "young", "new", "love" and "family" may indicate Netflix's emphasis on content for family-friendly audiences or younger demographics. \## RQ2.

### RQ2 Findings

Which genres are the most frequently produced on Netflix?

## RQ3.

### RQ3 Findings

# Methodology

# Discussion

# References

------------------------------------------------------------------------

# Code Appendix

```{r  codeAppend, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
