---
title: "[Stat 184] Final Project"
subtitle: Netflix Content Analyis 
author:
  - name: Allan Samoilovich
    corresponding: true
  - name: Junghyeon Sung
    corresponding: true
  - name: Taegwon Lee
    corresponding: true
date: 12/18/2024
format: 
  pdf:
    number-sections: true
    fig_caption: true
fig.show: "asis"
fig.align: "center"
echo: false
warning: false

---

# Introdcution

"Netflix" is the most popular media platform these days. It would not be an exaggeration to call it a pioneer of trends. Netflix has revolutionized the way people consume entertainment by introducing binge-watching, personalized recommendations, and producing high-quality original content. Its innovative approach to streaming has set a new standard for the industry, influencing how competitors deliver and create content. As media consumption continues to rise, we chose a netflix dataset to explore Netflix's trends.

## Dataset

We found our dataset from Kaggle and the dataset contains metadata about Netflix's movie and TV shows. It provides a comprehensive view of the Netflix's catalog and is widely used for research and analysis in media trends.

<Should be included>

1.  Describe the provenance of the data \* Where did you get the data \* Who collected the data \* For what purpose \* who/what make up the cases

2.  Explain how data meet the FAIR and/or CARE Principles

-   FAIR Principles:
    -   Findability: Other researchers should be able to find the data - i.e. name files descriptively based on the project. EX 'netflix_titles.csv'
    -   Accessibility: Other researchers should be able to access the data, within reason
    -   Interpretability: Data should use standardized formats/terminologies
    -   Reusability: Data should include detailed metadata to allow reuse

3.  Describe what attributes you'll focus your analysis on

-   Figures and Tables should have appropriate captions and appropriately cross-referenced in the body of your report.

## Data Wrangling

```{r}
# Load dataset
file_path <- "netflix_titles.csv"
netflix_data <- read.csv(file_path)
View(netflix_data)
```

```{r}
summary(netflix_data)
```

```{r}
# Necessary libraries for the entire code
library(dplyr)
library(tidyr)
library(ggplot2)
library(tm)
library(wordcloud)
library(psych)
library(knitr)

```

### Handling Missing Values

```{r}
# Find Missing Data
netflix_data[netflix_data == ""] <- NA
netflix_data[netflix_data == " "] <- NA

#colSums(is.na(netflix_data))

# Calculate the number of missing values in each column
missing_data <- colSums(is.na(netflix_data))
print(missing_data)
missing_data_df <- data.frame(Column = names(missing_data), MissingCount = missing_data)

ggplot(missing_data_df, aes(x = reorder(Column, -MissingCount), y = MissingCount, fill = MissingCount)) +
  geom_bar(stat = "identity") +  
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Missing Data Count by Column",
    x = "Columns",
    y = "Count of Missing Values"
  ) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme(legend.position = "none")
```

-   This graph shows that there are a lot of missing values in 'director' column.

```{r}
# Cleaning Missing Data
  # Replace missing values with "Unknown"
netflix_data$director[is.na(netflix_data$director)] <- "Unknown"
netflix_data$cast[is.na(netflix_data$cast)] <- "Unknown"
netflix_data$country[is.na(netflix_data$country)] <- "Unknown"

colSums(is.na(netflix_data))
```

```{r}
# Save the cleaned dataset
write.csv(netflix_data, "cleaned_netflix_data.csv", row.names = FALSE)
cleaned_data <- read.csv("cleaned_netflix_data.csv")
```

## Exploratory Data Analysis

### Movie vs. TV show
```{r}
#| fig-cap: "Content Trends by Bar graph"
#| fig.pos: "H"

# 1. Bar graph
ggplot(cleaned_data, aes(x = type, fill = type)) +
  geom_bar() +
  scale_fill_manual(values = c("#e50914", "#d4d4d4")) +
  labs(title = "Distribution of Content Types", x = "Type", y = "Count") +
  theme_minimal()
```


```{r}
#| fig-cap: "Content Trends by Pie Chart"
#| fig.pos: "H"

# 2. Pie graph
# Calculate percentages
content_type_dist <- cleaned_data %>%
  count(type) %>%
  mutate(percentage = n / sum(n) * 100)
# Visualization
ggplot(content_type_dist, aes(x = "", y = n, fill = type)) +
  geom_bar(width = 1, stat = "identity") +  # Bar chart as base
  coord_polar(theta = "y") +                # Convert to pie chart
  scale_fill_manual(values = c("#e50914", "#d4d4d4")) +
  geom_text(aes(label = paste0(round(percentage, 1), "%")),  # Add percentage labels
            position = position_stack(vjust = 0.5)) +     # Center labels
  labs(title = "Distribution of Content Types", x = NULL, y = NULL) +
  theme_minimal() +
  theme(axis.text.x = element_blank(),          # Remove x-axis labels
        axis.ticks = element_blank(),
        panel.grid = element_blank())           # Remove grid lines
```


```{r}

# Create the duration col
cleaned_data <- cleaned_data %>%
  mutate(duration_numeric = as.numeric(gsub("[^0-9]", "", duration)))

grouped_stats <- cleaned_data %>%
  group_by(type) %>%
  summarize(
    avg_duration = mean(duration_numeric, na.rm = TRUE),
    median_duration = median(duration_numeric, na.rm = TRUE),
    min_duration = min(duration_numeric, na.rm = TRUE),
    max_duration = max(duration_numeric, na.rm = TRUE),
    count = n()
  )

# Display the table using knitr::kable
kable(grouped_stats, caption = "Descriptive Statistics by Content Type")

```

### Duration

```{r}
#| fig-cap: "Distribution of movie durations in Netflix's catalog."
#| fig.pos: "H"


# Extract numeric values from duration
cleaned_data$duration_numeric <- as.numeric(gsub("[^0-9]", "", cleaned_data$duration))

# Distribution of duration for movies
movie_duration <- cleaned_data %>%
  filter(type == "Movie")

ggplot(movie_duration, aes(x = duration_numeric)) +
  geom_histogram(binwidth = 10, fill = "#e50914", color = "white") +
  labs(title = "Distribution of Movie Durations", x = "Duration (minutes)", y = "Count") +
  theme_minimal()

# Get descriptive statistics for numeric columns
describe_stats <- psych::describe(cleaned_data$duration_numeric)

# Display the results with kable
kable(describe_stats, 
      caption = "Descriptive Statistics for Movie Duration", 
      digits = 2, 
      align = "c")
```

-   The histogram highlights the most common movie lengths, with the majority clustering around 100 minutes.

### Yearly Content Addition

```{r}
#| fig-cap: "Yearly content addition on Netflix over time."
#| fig.pos: "H"


# Yearly trend
cleaned_data$year_added <- as.numeric(format(as.Date(cleaned_data$date_added, "%B %d, %Y"), "%Y"))

# Year-wise content count
yearly_content <- cleaned_data %>%
  filter(!is.na(year_added)) %>%
  count(year_added)

# Line plot
ggplot(yearly_content, aes(x = year_added, y = n)) +
  geom_line(color = "#e50914", size = 1.2) +
  geom_point(color = "#990033", size = 3) +
  labs(title = "Yearly Content Addition", x = "Year", y = "Content Count") +
  theme_minimal()

```

### Top 10 Genres

```{r}
#| fig-cap: "Genre Trends"
#| fig.pos: "H"

genre_data <- cleaned_data %>%
  separate_rows(listed_in, sep = ", ") %>%
  count(listed_in, sort = TRUE) %>%
  slice_max(order_by = n, n = 10)

# Plot top genres
ggplot(genre_data, aes(x = reorder(listed_in, n), y = n, fill = listed_in)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("#221f1f", "#b20710", "#e50914", "#f5f5f1",
                               "#8c8c8c", "#d4d4d4", "#660000", "#cc0000",
                               "#990033", "#330000")) +
  coord_flip() +
  labs(title = "Top 10 Genres on Netflix", x = "Genre", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

### Top 10 Countries

```{r}
#| fig-cap: "Top 10 Countries by Content Count"
#| fig.pos: "H"

# Find the top 15 countries 
top_countries <- cleaned_data %>%
  count(country, sort = TRUE) %>%
  top_n(10, n)
  # Visualization
custom_colors <- c("#e50914", "#221f1f", "#f5f5f1", "#8c8c8c",
                   "#660000", "#990033", "#330000",
                   "#ff5733", "#c70039", "#581845")

ggplot(top_countries, aes(x = reorder(country, n), y = n, fill = country)) + 
  geom_bar(stat = "identity") +
  scale_fill_manual(values = custom_colors) +  # Apply custom colors
  coord_flip() +
  labs(title = "Top 10 Countries by Content Count", x = "Country", y = "Count") +
  theme_minimal()+
  theme(legend.position = "none")
```

# Research Questions

## RQ1.What are the most frequently occurring keywords in the 'description' text of Netflix content?

```{r}
#| fig-cap: "Most frequent words in description"
#| fig.pos: "H"

# Extract the 'description' column and remove missing values
descriptions <- cleaned_data$description
descriptions <- na.omit(descriptions)

# Combine all descriptions into a single text
text <- paste(descriptions, collapse = " ")

# Create a corpus
corpus <- Corpus(VectorSource(text))

# Preprocess the text: remove punctuation, numbers, stopwords, and convert to lowercase
corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english"))

# Create a term-document matrix
tdm <- TermDocumentMatrix(corpus)
tdm_matrix <- as.matrix(tdm)

# Sum the frequency of words
word_freq <- sort(rowSums(tdm_matrix), decreasing = TRUE)

# Create a data frame of words and their frequencies
word_data <- data.frame(word = names(word_freq), freq = word_freq)

# Generate the word cloud
set.seed(123) # For consistent result
wordcloud(
  words = word_data$word,
  freq = word_data$freq,
  min.freq = 3, # Set minimum frequency for words to appear
  max.words = 150, # Set the maximum number of words to display
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)


## RQ2. Which genres are the most frequently produced on Netflix?

```
### RQ1 Findings

By analyzing frequency of text data efficiently, we used 'word cloud' graph. From research question 1, we found that the word "life" appears prominently, suggesting that many descriptions revolve around stories about human experiences. Similarly, the frequent occurrence of "young", "new", "love" and "family" may indicate Netflix's emphasis on content for family-friendly audiences or younger demographics.


## RQ2. Which genres are the most frequently produced on Netflix?

```{r}
#| fig-cap: "Top 10 most frequently produced genres on Netflix"
#| fig.pos: "H"

# Step 1: Extract and preprocess data
# Load the 'listed_in' column, which contains genre information, and remove missing values
genres <- cleaned_data$listed_in
genres <- na.omit(genres)

# Step 2: Split multiple genres in a single entry into separate rows
# Split genres by comma and trim whitespace
genres_data <- data.frame(genres = unlist(strsplit(as.character(genres), ",\\s*")))

# Step 3: Count the frequency of each genre
genre_frequency <- genres_data %>%
  group_by(genres) %>%
  summarise(Frequency = n()) %>%
  arrange(desc(Frequency))

# Display the top 10 genres
print(head(genre_frequency, 10))

# Step 4: Visualize the top genres
ggplot(data = head(genre_frequency, 10), aes(x = reorder(genres, Frequency), y = Frequency)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Top 10 Most Frequently Produced Genres on Netflix",
       x = "Genre",
       y = "Frequency") +
  theme_minimal()

# Step 5: Save the result for further use
write.csv(genre_frequency, "netflix_genre_frequency.csv", row.names = FALSE)

# Additional Exploration: Genres across regions
# If there are columns related to country or region, we can analyze genre distributions by region
if ("country" %in% colnames(cleaned_data)) {
  genre_region <- cleaned_data %>%
    select(listed_in, country) %>%
    separate_rows(listed_in, sep = ",\\s*") %>%
    group_by(country, listed_in) %>%
    summarise(Frequency = n()) %>%
    arrange(country, desc(Frequency))

  # Save region-specific genre data
  write.csv(genre_region, "netflix_genre_by_region.csv", row.names = FALSE)
}

```

## RQ3. Is there a correlation between specific directors and certain keywords in Netflix descriptions
```{r}
#| fig-cap: "Corrlation between directors and keywords"
#| fig.pos: "H"


# Preprocessing to find correlation between directors and keywords in descriptions

# Step 1: Extract and preprocess data
cleaned_data$description <- tolower(cleaned_data$description) 
cleaned_data$description <- gsub("[[:punct:]]", "", cleaned_data$description) 
cleaned_data$description <- gsub("[[:digit:]]", "", cleaned_data$description) 
cleaned_data$description <- gsub("\\s+", " ", cleaned_data$description) 
cleaned_data$description <- trimws(cleaned_data$description) 

# Remove stopwords from descriptions
stop_words <- stopwords("en") 
cleaned_data$processed_description <- sapply(cleaned_data$description, function(x) {
  paste(setdiff(unlist(strsplit(x, " ")), stop_words), collapse = " ")
})

# Step 2: Create a mapping of directors to their descriptions
director_keywords <- cleaned_data %>%
  filter(director != "Unknown") %>%
  group_by(director) %>%
  summarise(all_descriptions = paste(processed_description, collapse = " ")) %>%
  ungroup()

# Step 3: Tokenize and extract keywords for each director
corpus <- Corpus(VectorSource(director_keywords$all_descriptions))

# Further preprocessing
corpus <- corpus %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english"))

# Create a Term-Document Matrix
tdm <- TermDocumentMatrix(corpus)
tdm_matrix <- as.matrix(tdm)

# Find the most frequent terms for each director
director_keywords$top_keywords <- apply(tdm_matrix, 2, function(x) {
  top_terms <- names(sort(x, decreasing = TRUE)[1:10]) # Top 10 terms
  paste(top_terms, collapse = ", ")
})

# Step 4: Visualize the correlations
selected_director <- "Quentin Tarantino" # Replace with any director of interest

# Find the corresponding keywords
if (selected_director %in% director_keywords$director) {
  selected_keywords <- tdm_matrix[, which(director_keywords$director == selected_director)]
  selected_keywords <- selected_keywords[selected_keywords > 0]
  
  # Create a word cloud
  wordcloud(
    words = names(selected_keywords),
    freq = selected_keywords,
    min.freq = 1,
    max.words = 100,
    random.order = FALSE,
    colors = brewer.pal(8, "Dark2")
  )
} else {
  cat("Director not found in the dataset.")
}

# Step 5: Save results
write.csv(director_keywords, "director_keywords.csv", row.names = FALSE)
```

### RQ3 Findings
By analyizing the results of the data, we can concur that certain directors do end up being associated with specific directors. A notable example is the director Quentin Tarantino and the tendency for the descriptions of his movies to include words such as "violence" or "revenge". Coincidentally, the type of movies he generally produces are actions films. From this we can gleam that certain directors will have a preference for certain genres of films.


# Discussion
The length of movies seems to be an hour and 30 minutes long at average because it is neither too long nor too short for the average person to binge watch on any given day. Most media on Netflix is movies because that seems to be the most easily bingeable type of media. Combined with the hour and 30 minute length, this means that Netflix's general plan is to get viewers to binge as many relatively short movies as possible. Films have historically also been known to cost less to produce than shows, as they take up less time to produce and are as a result much easier to produce more of than multi-episode shows. A large amount of media on Netflix are family friendly films because it is the easiest genre to market to the most amount of people and can provide much broader appeal with its diverse list of easy to understand and emotional themes.


# References
Bansal, S. (2023). *Netflix Movies and TV Shows Dataset*. [https://www.kaggle.com/datasets/shivamb/netflix-shows](https://www.kaggle.com/datasets/shivamb/netflix-shows)
------------------------------------------------------------------------

# Code Appendix

```{r  codeAppend, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
